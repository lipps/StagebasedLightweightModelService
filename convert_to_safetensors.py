#!/usr/bin/env python3
"""
å°† pytorch_model.bin è½¬æ¢ä¸º model.safetensors æ ¼å¼
è§£å†³ PyTorch 2.3.0 çš„å®‰å…¨æ¼æ´é—®é¢˜
"""

import os
import torch
from safetensors.torch import save_file

MODEL_DIR = "/opt/bge-m3/models/bge-m3"
BIN_FILE = os.path.join(MODEL_DIR, "pytorch_model.bin")
SAFETENSORS_FILE = os.path.join(MODEL_DIR, "model.safetensors")

print("ğŸ”„ å¼€å§‹è½¬æ¢æ¨¡å‹æ ¼å¼...")
print(f"ğŸ“‚ æºæ–‡ä»¶: {BIN_FILE}")
print(f"ğŸ“‚ ç›®æ ‡æ–‡ä»¶: {SAFETENSORS_FILE}")

# æ£€æŸ¥æºæ–‡ä»¶
if not os.path.exists(BIN_FILE):
    raise FileNotFoundError(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {BIN_FILE}")

# ä½¿ç”¨æ—§ç‰ˆ torch.load åŠ è½½ï¼ˆ2.3.0 æ”¯æŒï¼‰
print("\nğŸ“¥ åŠ è½½ PyTorch æƒé‡...")
# ç¦ç”¨å®‰å…¨æ£€æŸ¥ä»¥ä¾¿åœ¨æ—§ç‰ˆæœ¬ä¸­åŠ è½½
import warnings
warnings.filterwarnings('ignore')

# ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡è·³è¿‡å®‰å…¨æ£€æŸ¥
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'

try:
    # ç›´æ¥ä½¿ç”¨ torch.loadï¼ˆåœ¨ transformers æ£€æŸ¥ä¹‹å‰ï¼‰
    state_dict = torch.load(BIN_FILE, map_location='cpu', weights_only=False)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(state_dict)} ä¸ªæƒé‡å¼ é‡")

    # è½¬æ¢ä¸º safetensors
    print("\nğŸ’¾ ä¿å­˜ä¸º safetensors æ ¼å¼...")
    save_file(state_dict, SAFETENSORS_FILE)

    # éªŒè¯æ–‡ä»¶å¤§å°
    bin_size = os.path.getsize(BIN_FILE) / (1024**3)
    safe_size = os.path.getsize(SAFETENSORS_FILE) / (1024**3)

    print(f"\nâœ… è½¬æ¢å®Œæˆ!")
    print(f"ğŸ“Š åŸå§‹å¤§å°: {bin_size:.2f} GB")
    print(f"ğŸ“Š æ–°æ–‡ä»¶å¤§å°: {safe_size:.2f} GB")
    print(f"ğŸ“Š å·®å¼‚: {((safe_size - bin_size) / bin_size * 100):+.1f}%")

    print("\nâš ï¸  å»ºè®®æ“ä½œ:")
    print("1. éªŒè¯æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½")
    print("2. æµ‹è¯•é€šè¿‡åå¯åˆ é™¤åŸå§‹ pytorch_model.bin æ–‡ä»¶")
    print(f"   rm {BIN_FILE}")

except Exception as e:
    print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
    raise
