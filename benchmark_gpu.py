#!/usr/bin/env python3
"""
BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯•
å¯¹æ¯” CPU å’Œ GPU æ¨¡å¼çš„æ€§èƒ½å·®å¼‚
"""

import time
import torch
from transformers import AutoTokenizer, AutoModel
from typing import List
import os

print("="*80)
print("  BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯•")
print("="*80)

MODEL_ID = "/opt/bge-m3/models/bge-m3"

# æµ‹è¯•é…ç½®
TEST_CONFIGS = [
    {"name": "å°æ‰¹é‡çŸ­æ–‡æœ¬", "texts": ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"] * 10, "batch_size": 8, "max_length": 128},
    {"name": "ä¸­æ‰¹é‡ä¸­æ–‡æœ¬", "texts": [f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºè¯„ä¼°æ€§èƒ½" * 5 for i in range(50)], "batch_size": 32, "max_length": 256},
    {"name": "å¤§æ‰¹é‡é•¿æ–‡æœ¬", "texts": [f"è¿™æ˜¯ç¬¬{i}ä¸ªè¾ƒé•¿çš„æµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«æ›´å¤šå†…å®¹ç”¨äºå…¨é¢è¯„ä¼°ç³»ç»Ÿæ€§èƒ½è¡¨ç°" * 10 for i in range(100)], "batch_size": 64, "max_length": 512},
]


def mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Mean Pooling"""
    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)
    summed = (last_hidden_state * mask).sum(dim=1)
    counts = mask.sum(dim=1).clamp(min=1e-9)
    return summed / counts


@torch.inference_mode()
def encode(tokenizer, model, texts: List[str], device: str, dtype: torch.dtype,
           max_length: int, batch_size: int) -> List[List[float]]:
    """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
    all_vecs = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]

        inputs = tokenizer(
            batch,
            padding="longest",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        out = model(**inputs, return_dict=True)
        vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
        vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
        all_vecs.extend(vecs.detach().float().cpu().tolist())

    return all_vecs


def benchmark(device: str, dtype: torch.dtype, threads: int = None):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    print(f"\n{'='*80}")
    print(f"  æµ‹è¯•æ¨¡å¼: {device.upper()} ({'FP16' if dtype == torch.float16 else 'FP32'})")
    if threads:
        print(f"  CPU çº¿ç¨‹æ•°: {threads}")
    print("="*80)

    # é…ç½® CPU çº¿ç¨‹
    if threads:
        torch.set_num_threads(threads)

    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, local_files_only=True)
    model = AutoModel.from_pretrained(MODEL_ID, torch_dtype=dtype, local_files_only=True)
    model.to(device)
    model.eval()
    load_time = time.time() - start
    print(f"  âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({load_time:.2f}s)")

    # æ¨¡å‹é¢„çƒ­
    print("\næ¨¡å‹é¢„çƒ­...")
    _ = encode(tokenizer, model, ["é¢„çƒ­æµ‹è¯•"] * 4, device, dtype, 128, 4)
    if device == "cuda":
        torch.cuda.synchronize()
    print("  âœ… é¢„çƒ­å®Œæˆ")

    # è¿è¡Œæµ‹è¯•
    results = []
    for config in TEST_CONFIGS:
        print(f"\n{'â”€'*80}")
        print(f"  æµ‹è¯•åœºæ™¯: {config['name']}")
        print(f"  æ–‡æœ¬æ•°é‡: {len(config['texts'])}")
        print(f"  æ‰¹å¤„ç†å¤§å°: {config['batch_size']}")
        print(f"  æœ€å¤§é•¿åº¦: {config['max_length']}")
        print("â”€"*80)

        # è¿è¡Œæµ‹è¯•
        start = time.time()
        embeddings = encode(
            tokenizer, model, config['texts'], device, dtype,
            config['max_length'], config['batch_size']
        )
        if device == "cuda":
            torch.cuda.synchronize()
        elapsed = time.time() - start

        # è®¡ç®—æŒ‡æ ‡
        count = len(embeddings)
        throughput = count / elapsed
        latency = elapsed * 1000
        avg_latency = latency / count

        print(f"\n  ç»“æœ:")
        print(f"    æ€»è€—æ—¶: {latency:.2f}ms")
        print(f"    å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms/æ–‡æœ¬")
        print(f"    ååé‡: {throughput:.2f} æ–‡æœ¬/ç§’")

        results.append({
            "name": config["name"],
            "count": count,
            "elapsed": elapsed,
            "throughput": throughput,
            "latency": latency
        })

    # æ¸…ç†
    del model, tokenizer
    if device == "cuda":
        torch.cuda.empty_cache()

    return results


def main():
    """ä¸»å‡½æ•°"""
    all_results = {}

    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print(f"\nCUDA å¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")

    if cuda_available:
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")

    # æµ‹è¯• 1: CPU å•çº¿ç¨‹ï¼ˆåŸç‰ˆé…ç½®ï¼‰
    print("\n\n" + "="*80)
    print("  ç¬¬ 1 é¡¹æµ‹è¯•: CPU å•çº¿ç¨‹ï¼ˆåŸç‰ˆåŸºçº¿ï¼‰")
    print("="*80)
    all_results["CPU å•çº¿ç¨‹"] = benchmark("cpu", torch.float32, threads=1)

    # æµ‹è¯• 2: CPU å¤šçº¿ç¨‹ï¼ˆPro ç‰ˆ CPU ä¼˜åŒ–ï¼‰
    print("\n\n" + "="*80)
    print("  ç¬¬ 2 é¡¹æµ‹è¯•: CPU å¤šçº¿ç¨‹ï¼ˆPro ç‰ˆä¼˜åŒ–ï¼‰")
    print("="*80)
    all_results["CPU å¤šçº¿ç¨‹"] = benchmark("cpu", torch.float32, threads=8)

    # æµ‹è¯• 3: GPU FP32
    if cuda_available:
        print("\n\n" + "="*80)
        print("  ç¬¬ 3 é¡¹æµ‹è¯•: GPU FP32")
        print("="*80)
        all_results["GPU FP32"] = benchmark("cuda", torch.float32)

        # æµ‹è¯• 4: GPU FP16ï¼ˆæœ€ä¼˜é…ç½®ï¼‰
        print("\n\n" + "="*80)
        print("  ç¬¬ 4 é¡¹æµ‹è¯•: GPU FP16ï¼ˆæœ€ä¼˜é…ç½®ï¼‰")
        print("="*80)
        all_results["GPU FP16"] = benchmark("cuda", torch.float16)

    # æ€§èƒ½å¯¹æ¯”è¡¨
    print("\n\n" + "="*80)
    print("  æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*80)

    # æå–ç¬¬äºŒä¸ªæµ‹è¯•åœºæ™¯çš„æ•°æ®ï¼ˆä¸­ç­‰è§„æ¨¡ï¼‰
    baseline_name = "CPU å•çº¿ç¨‹"
    baseline_throughput = all_results[baseline_name][1]["throughput"]

    print(f"\nåŸºçº¿ï¼ˆ{baseline_name}ï¼‰: {baseline_throughput:.2f} æ–‡æœ¬/ç§’")
    print("\n" + "â”Œ" + "â”€"*20 + "â”¬" + "â”€"*15 + "â”¬" + "â”€"*15 + "â”¬" + "â”€"*15 + "â”")
    print(f"â”‚ {'é…ç½®':<18} â”‚ {'åå(/s)':>13} â”‚ {'å»¶è¿Ÿ(ms)':>13} â”‚ {'æå‡å€æ•°':>13} â”‚")
    print("â”œ" + "â”€"*20 + "â”¼" + "â”€"*15 + "â”¼" + "â”€"*15 + "â”¼" + "â”€"*15 + "â”¤")

    for config_name, results in all_results.items():
        # ä½¿ç”¨ä¸­ç­‰è§„æ¨¡æµ‹è¯•çš„æ•°æ®
        result = results[1]
        throughput = result["throughput"]
        latency = result["latency"]
        speedup = throughput / baseline_throughput

        print(f"â”‚ {config_name:<18} â”‚ {throughput:>13.2f} â”‚ {latency:>13.2f} â”‚ {speedup:>12.2f}x â”‚")

    print("â””" + "â”€"*20 + "â”´" + "â”€"*15 + "â”´" + "â”€"*15 + "â”´" + "â”€"*15 + "â”˜")

    # æ˜¾å­˜ä½¿ç”¨ï¼ˆå¦‚æœæœ‰ GPUï¼‰
    if cuda_available:
        print(f"\næ˜¾å­˜ä½¿ç”¨:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
        print(f"  æœ€å¤§å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.2f} MB")

    print("\n" + "="*80)
    print("  æµ‹è¯•å®Œæˆï¼")
    print("="*80)

    # æ¨èé…ç½®
    print("\nğŸ“Š æ¨èé…ç½®:")
    if cuda_available:
        best_config = max(all_results.items(), key=lambda x: x[1][1]["throughput"])
        print(f"  æœ€ä½³æ€§èƒ½: {best_config[0]} ({best_config[1][1]['throughput']:.2f} æ–‡æœ¬/ç§’)")
        print(f"  ğŸš€ æ€§èƒ½æå‡: {best_config[1][1]['throughput'] / baseline_throughput:.1f}x")
    else:
        print("  æ¨èä½¿ç”¨ CPU å¤šçº¿ç¨‹æ¨¡å¼ï¼ˆæ—  GPU å¯ç”¨ï¼‰")


if __name__ == "__main__":
    main()
