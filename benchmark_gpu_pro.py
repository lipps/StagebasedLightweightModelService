#!/usr/bin/env python3
"""
BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯• - å¢å¼ºç‰ˆ
è¦†ç›–åœºæ™¯ï¼š
  1. ä¸åŒé•¿åº¦æ–‡æœ¬æ‰¹å¤„ç†åå (256/1024/4096 tokens)
  2. FP16 å®‰å…¨ batch size æ¢æµ‹ (OOM è¾¹ç•Œ)
  3. æ··åˆé•¿åº¦åœºæ™¯ (çŸ­å¥+é•¿å¥æ··æ‰¹, tail latency)
  4. P95/P99 å»¶è¿ŸæŒ‡æ ‡
  5. Tokenize æ—¶é—´å æ¯”æ‹†è§£
"""

import gc
import time
import random
import statistics
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, field

import torch
from transformers import AutoTokenizer, AutoModel

# ==================== é…ç½® ====================

MODEL_ID = "/opt/bge-m3/models/bge-m3"

# ç›®æ ‡ token é•¿åº¦çš„æ–‡æœ¬ç”Ÿæˆæ¨¡æ¿
# ä¸­æ–‡çº¦ 1.5 char/token, è‹±æ–‡çº¦ 4 char/token
TEXT_TEMPLATES = {
    "short": "è¿™æ˜¯ä¸€æ®µç®€çŸ­çš„æµ‹è¯•æ–‡æœ¬ã€‚",  # ~10 tokens
    "medium": "äººå·¥æ™ºèƒ½æ­£åœ¨æ·±åˆ»æ”¹å˜ç€æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶ï¼Œä»åŒ»ç–—è¯Šæ–­åˆ°é‡‘èåˆ†æã€‚" * 3,  # ~50 tokens
    "long": ("å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œé€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œ"
             "èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ï¼Œåœ¨é—®ç­”ã€ç¿»è¯‘ã€æ‘˜è¦ã€å¯¹è¯ç­‰ä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚") * 10,  # ~200 tokens
}


@dataclass
class BenchmarkResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    name: str
    text_count: int
    batch_size: int
    max_length: int
    total_time_ms: float
    tokenize_time_ms: float
    encode_time_ms: float
    throughput: float  # texts/sec
    avg_latency_ms: float
    latencies: List[float] = field(default_factory=list)  # æ¯ä¸ª batch çš„å»¶è¿Ÿ
    memory_mb: float = 0.0
    peak_memory_mb: float = 0.0


@dataclass
class LatencyStats:
    """å»¶è¿Ÿç»Ÿè®¡"""
    min_ms: float
    max_ms: float
    avg_ms: float
    p50_ms: float
    p90_ms: float
    p95_ms: float
    p99_ms: float
    std_ms: float


def generate_texts_by_tokens(tokenizer, target_tokens: int, count: int) -> List[str]:
    """ç”ŸæˆæŒ‡å®š token é•¿åº¦çš„æ–‡æœ¬"""
    # åŸºç¡€æ–‡æœ¬å—
    base_text = ("è¿™æ˜¯ä¸€æ®µç”¨äºæ€§èƒ½æµ‹è¯•çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«äº†å¤šç§ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ã€‚"
                 "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚")

    # ä¼°ç®—æ¯ä¸ªå­—ç¬¦çš„å¹³å‡ token æ•°
    sample_tokens = len(tokenizer.encode(base_text, add_special_tokens=False))
    chars_per_token = len(base_text) / sample_tokens

    # ç”Ÿæˆç›®æ ‡é•¿åº¦çš„æ–‡æœ¬
    target_chars = int(target_tokens * chars_per_token * 1.1)  # ç¨å¾®å¤šä¸€ç‚¹ç¡®ä¿å¤Ÿé•¿

    texts = []
    for i in range(count):
        # é‡å¤åŸºç¡€æ–‡æœ¬ç›´åˆ°è¾¾åˆ°ç›®æ ‡é•¿åº¦
        repeated = (base_text + f"[{i}]") * (target_chars // len(base_text) + 1)
        text = repeated[:target_chars]
        texts.append(text)

    return texts


def generate_mixed_length_texts(tokenizer, count: int) -> Tuple[List[str], List[int]]:
    """ç”Ÿæˆæ··åˆé•¿åº¦çš„æ–‡æœ¬ (æ¨¡æ‹ŸçœŸå®åœºæ™¯)
    åˆ†å¸ƒ: 40% çŸ­å¥(<128), 40% ä¸­å¥(128-512), 20% é•¿å¥(512-2048)
    """
    texts = []
    lengths = []

    short_count = int(count * 0.4)
    medium_count = int(count * 0.4)
    long_count = count - short_count - medium_count

    # çŸ­å¥
    short_texts = generate_texts_by_tokens(tokenizer, 64, short_count)
    texts.extend(short_texts)
    lengths.extend([64] * short_count)

    # ä¸­å¥
    medium_texts = generate_texts_by_tokens(tokenizer, 256, medium_count)
    texts.extend(medium_texts)
    lengths.extend([256] * medium_count)

    # é•¿å¥
    long_texts = generate_texts_by_tokens(tokenizer, 1024, long_count)
    texts.extend(long_texts)
    lengths.extend([1024] * long_count)

    # æ‰“ä¹±é¡ºåº
    combined = list(zip(texts, lengths))
    random.shuffle(combined)
    texts, lengths = zip(*combined)

    return list(texts), list(lengths)


def mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Mean Pooling"""
    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)
    summed = (last_hidden_state * mask).sum(dim=1)
    counts = mask.sum(dim=1).clamp(min=1e-9)
    return summed / counts


def compute_latency_stats(latencies: List[float]) -> LatencyStats:
    """è®¡ç®—å»¶è¿Ÿç»Ÿè®¡æŒ‡æ ‡"""
    if not latencies:
        return LatencyStats(0, 0, 0, 0, 0, 0, 0, 0)

    sorted_latencies = sorted(latencies)
    n = len(sorted_latencies)

    def percentile(p: float) -> float:
        idx = int(n * p / 100)
        return sorted_latencies[min(idx, n - 1)]

    return LatencyStats(
        min_ms=min(latencies),
        max_ms=max(latencies),
        avg_ms=statistics.mean(latencies),
        p50_ms=percentile(50),
        p90_ms=percentile(90),
        p95_ms=percentile(95),
        p99_ms=percentile(99),
        std_ms=statistics.stdev(latencies) if len(latencies) > 1 else 0
    )


@torch.inference_mode()
def benchmark_with_breakdown(
    tokenizer,
    model,
    texts: List[str],
    device: str,
    dtype: torch.dtype,
    max_length: int,
    batch_size: int,
    name: str = "test"
) -> BenchmarkResult:
    """å¸¦æ—¶é—´æ‹†è§£çš„åŸºå‡†æµ‹è¯•"""

    all_vecs = []
    batch_latencies = []
    total_tokenize_time = 0.0
    total_encode_time = 0.0

    # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()

    start_total = time.perf_counter()

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_start = time.perf_counter()

        # ========== Tokenize é˜¶æ®µ ==========
        tokenize_start = time.perf_counter()
        inputs = tokenizer(
            batch,
            padding="longest",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if device == "cuda":
            torch.cuda.synchronize()
        tokenize_end = time.perf_counter()
        total_tokenize_time += (tokenize_end - tokenize_start) * 1000

        # ========== Encode é˜¶æ®µ ==========
        encode_start = time.perf_counter()
        out = model(**inputs, return_dict=True)
        vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
        vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
        if device == "cuda":
            torch.cuda.synchronize()
        encode_end = time.perf_counter()
        total_encode_time += (encode_end - encode_start) * 1000

        all_vecs.extend(vecs.detach().float().cpu().tolist())

        batch_end = time.perf_counter()
        batch_latencies.append((batch_end - batch_start) * 1000)

    if device == "cuda":
        torch.cuda.synchronize()

    end_total = time.perf_counter()
    total_time_ms = (end_total - start_total) * 1000

    # æ˜¾å­˜ç»Ÿè®¡
    memory_mb = 0.0
    peak_memory_mb = 0.0
    if device == "cuda":
        memory_mb = torch.cuda.memory_allocated() / 1024**2
        peak_memory_mb = torch.cuda.max_memory_allocated() / 1024**2

    count = len(all_vecs)
    throughput = count / (total_time_ms / 1000) if total_time_ms > 0 else 0

    return BenchmarkResult(
        name=name,
        text_count=count,
        batch_size=batch_size,
        max_length=max_length,
        total_time_ms=total_time_ms,
        tokenize_time_ms=total_tokenize_time,
        encode_time_ms=total_encode_time,
        throughput=throughput,
        avg_latency_ms=total_time_ms / count if count > 0 else 0,
        latencies=batch_latencies,
        memory_mb=memory_mb,
        peak_memory_mb=peak_memory_mb
    )


def find_safe_batch_size(
    tokenizer,
    model,
    device: str,
    dtype: torch.dtype,
    max_length: int,
    start_batch: int = 1,
    max_batch: int = 512
) -> Tuple[int, float]:
    """äºŒåˆ†æŸ¥æ‰¾å®‰å…¨çš„æœ€å¤§ batch size (é¿å… OOM)

    è¿”å›: (safe_batch_size, peak_memory_mb)
    """
    if device != "cuda":
        return max_batch, 0.0

    print(f"\n  ğŸ” æ¢æµ‹ max_length={max_length} çš„å®‰å…¨ batch size...")

    # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
    test_texts = generate_texts_by_tokens(tokenizer, max_length, max_batch)

    safe_batch = start_batch
    safe_memory = 0.0

    # äºŒåˆ†æŸ¥æ‰¾
    low, high = start_batch, max_batch

    while low <= high:
        mid = (low + high) // 2

        try:
            # æ¸…ç†æ˜¾å­˜
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

            # å°è¯•è¿è¡Œ
            batch_texts = test_texts[:mid]
            inputs = tokenizer(
                batch_texts,
                padding="longest",
                truncation=True,
                max_length=max_length,
                return_tensors="pt",
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.inference_mode():
                out = model(**inputs, return_dict=True)
                vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
                _ = vecs.cpu()

            torch.cuda.synchronize()
            peak_mem = torch.cuda.max_memory_allocated() / 1024**2

            # æˆåŠŸï¼Œè®°å½•å¹¶å°è¯•æ›´å¤§çš„ batch
            safe_batch = mid
            safe_memory = peak_mem
            print(f"    âœ… batch={mid} æˆåŠŸï¼Œå³°å€¼æ˜¾å­˜ {peak_mem:.1f}MB")
            low = mid + 1

        except torch.cuda.OutOfMemoryError:
            print(f"    âŒ batch={mid} OOM")
            high = mid - 1
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"    âš ï¸  batch={mid} é”™è¯¯: {e}")
            high = mid - 1

    # å®‰å…¨è¾¹ç•Œï¼šé™ä½ 10%
    safe_batch = int(safe_batch * 0.9)

    return safe_batch, safe_memory


def print_result_table(results: List[BenchmarkResult], title: str):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    print(f"\n{'='*100}")
    print(f"  {title}")
    print('='*100)

    # è¡¨å¤´
    print(f"\n{'åœºæ™¯':<20} {'æ•°é‡':>8} {'Batch':>8} {'MaxLen':>8} "
          f"{'åå(/s)':>12} {'å»¶è¿Ÿ(ms)':>12} {'æ˜¾å­˜(MB)':>12}")
    print("-" * 100)

    for r in results:
        print(f"{r.name:<20} {r.text_count:>8} {r.batch_size:>8} {r.max_length:>8} "
              f"{r.throughput:>12.2f} {r.avg_latency_ms:>12.2f} {r.peak_memory_mb:>12.1f}")


def print_latency_breakdown(results: List[BenchmarkResult]):
    """æ‰“å°å»¶è¿Ÿæ‹†è§£"""
    print(f"\n{'='*100}")
    print("  æ—¶é—´æ‹†è§£åˆ†æ (Tokenize vs Encode)")
    print('='*100)

    print(f"\n{'åœºæ™¯':<20} {'æ€»æ—¶é—´(ms)':>12} {'Tokenize(ms)':>14} {'Encode(ms)':>12} "
          f"{'Tokenize%':>10} {'Encode%':>10}")
    print("-" * 100)

    for r in results:
        tok_pct = (r.tokenize_time_ms / r.total_time_ms * 100) if r.total_time_ms > 0 else 0
        enc_pct = (r.encode_time_ms / r.total_time_ms * 100) if r.total_time_ms > 0 else 0

        print(f"{r.name:<20} {r.total_time_ms:>12.2f} {r.tokenize_time_ms:>14.2f} "
              f"{r.encode_time_ms:>12.2f} {tok_pct:>9.1f}% {enc_pct:>9.1f}%")


def print_percentile_stats(results: List[BenchmarkResult]):
    """æ‰“å° P50/P90/P95/P99 å»¶è¿Ÿ"""
    print(f"\n{'='*100}")
    print("  æ‰¹æ¬¡å»¶è¿Ÿåˆ†ä½æ•° (P50/P90/P95/P99)")
    print('='*100)

    print(f"\n{'åœºæ™¯':<20} {'Min(ms)':>10} {'P50(ms)':>10} {'P90(ms)':>10} "
          f"{'P95(ms)':>10} {'P99(ms)':>10} {'Max(ms)':>10} {'StdDev':>10}")
    print("-" * 100)

    for r in results:
        stats = compute_latency_stats(r.latencies)
        print(f"{r.name:<20} {stats.min_ms:>10.2f} {stats.p50_ms:>10.2f} "
              f"{stats.p90_ms:>10.2f} {stats.p95_ms:>10.2f} {stats.p99_ms:>10.2f} "
              f"{stats.max_ms:>10.2f} {stats.std_ms:>10.2f}")


def main():
    """ä¸»å‡½æ•°"""
    print("="*100)
    print("  BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯• - å¢å¼ºç‰ˆ")
    print("="*100)

    # æ£€æŸ¥ CUDA
    cuda_available = torch.cuda.is_available()
    print(f"\nCUDA å¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")

    if cuda_available:
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"æ˜¾å­˜æ€»é‡: {gpu_mem:.1f} GB")

    device = "cuda" if cuda_available else "cpu"
    dtype = torch.float16 if cuda_available else torch.float32

    # åŠ è½½æ¨¡å‹
    print(f"\nåŠ è½½æ¨¡å‹ ({device.upper()}, {'FP16' if dtype == torch.float16 else 'FP32'})...")
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, local_files_only=True)
    model = AutoModel.from_pretrained(MODEL_ID, torch_dtype=dtype, local_files_only=True)
    model.to(device)
    model.eval()
    print(f"  âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({time.time() - start:.2f}s)")

    # é¢„çƒ­
    print("\næ¨¡å‹é¢„çƒ­...")
    _ = benchmark_with_breakdown(
        tokenizer, model, ["é¢„çƒ­æµ‹è¯•"] * 8, device, dtype, 128, 8, "warmup"
    )
    print("  âœ… é¢„çƒ­å®Œæˆ")

    all_results = []

    # ==================== æµ‹è¯• 1: ä¸åŒé•¿åº¦æ–‡æœ¬åå ====================
    print("\n" + "="*100)
    print("  æµ‹è¯• 1: ä¸åŒé•¿åº¦æ–‡æœ¬æ‰¹å¤„ç†åå (256/1024/4096 tokens)")
    print("="*100)

    length_configs = [
        {"target_tokens": 256, "count": 200, "batch_size": 64, "max_length": 512},
        {"target_tokens": 1024, "count": 100, "batch_size": 32, "max_length": 1536},
        {"target_tokens": 4096, "count": 50, "batch_size": 8, "max_length": 4608},
    ]

    length_results = []
    for cfg in length_configs:
        print(f"\n  ğŸ“Š æµ‹è¯• ~{cfg['target_tokens']} tokens æ–‡æœ¬...")
        texts = generate_texts_by_tokens(tokenizer, cfg["target_tokens"], cfg["count"])

        # éªŒè¯å®é™… token é•¿åº¦
        sample_len = len(tokenizer.encode(texts[0], add_special_tokens=True))
        print(f"     å®é™…æ ·æœ¬é•¿åº¦: ~{sample_len} tokens")

        result = benchmark_with_breakdown(
            tokenizer, model, texts, device, dtype,
            cfg["max_length"], cfg["batch_size"],
            f"~{cfg['target_tokens']}tok"
        )
        length_results.append(result)
        print(f"     åå: {result.throughput:.2f}/s, å»¶è¿Ÿ: {result.avg_latency_ms:.2f}ms")

    all_results.extend(length_results)
    print_result_table(length_results, "ä¸åŒé•¿åº¦æ–‡æœ¬ååå¯¹æ¯”")

    # ==================== æµ‹è¯• 2: å®‰å…¨ Batch Size æ¢æµ‹ ====================
    if cuda_available:
        print("\n" + "="*100)
        print("  æµ‹è¯• 2: FP16 å®‰å…¨ Batch Size æ¢æµ‹ (OOM è¾¹ç•Œ)")
        print("="*100)

        safe_batch_results = {}
        for max_len in [512, 1024, 2048, 4096]:
            safe_batch, peak_mem = find_safe_batch_size(
                tokenizer, model, device, dtype, max_len,
                start_batch=1, max_batch=256
            )
            safe_batch_results[max_len] = (safe_batch, peak_mem)

        print(f"\n  ğŸ“‹ å®‰å…¨ Batch Size æ¨è (å« 10% å®‰å…¨è¾¹ç•Œ):")
        print(f"\n  {'MaxLength':>12} â”‚ {'SafeBatch':>12} â”‚ {'PeakMem(MB)':>14}")
        print(f"  {'-'*12}â”€â”¼â”€{'-'*12}â”€â”¼â”€{'-'*14}")
        for max_len, (batch, mem) in safe_batch_results.items():
            print(f"  {max_len:>12} â”‚ {batch:>12} â”‚ {mem:>14.1f}")

    # ==================== æµ‹è¯• 3: æ··åˆé•¿åº¦åœºæ™¯ ====================
    print("\n" + "="*100)
    print("  æµ‹è¯• 3: æ··åˆé•¿åº¦åœºæ™¯ (çœŸå®åˆ†å¸ƒ: 40%çŸ­+40%ä¸­+20%é•¿)")
    print("="*100)

    mixed_texts, mixed_lengths = generate_mixed_length_texts(tokenizer, 200)
    print(f"\n  æ–‡æœ¬åˆ†å¸ƒ: çŸ­(~64tok): {mixed_lengths.count(64)}, "
          f"ä¸­(~256tok): {mixed_lengths.count(256)}, é•¿(~1024tok): {mixed_lengths.count(1024)}")

    # æµ‹è¯•ä¸åŒ batch size ä¸‹çš„è¡¨ç°
    mixed_results = []
    for batch_size in [16, 32, 64]:
        result = benchmark_with_breakdown(
            tokenizer, model, mixed_texts, device, dtype,
            1536, batch_size, f"æ··åˆ-B{batch_size}"
        )
        mixed_results.append(result)

    all_results.extend(mixed_results)
    print_result_table(mixed_results, "æ··åˆé•¿åº¦åœºæ™¯å¯¹æ¯”")

    # ==================== æµ‹è¯• 4: é«˜å¹¶å‘å‹åŠ›æµ‹è¯• (P95/P99) ====================
    print("\n" + "="*100)
    print("  æµ‹è¯• 4: é«˜å¹¶å‘å‹åŠ›æµ‹è¯• (å¤šè½®è¿­ä»£, æ”¶é›† P95/P99)")
    print("="*100)

    # ç”¨ä¸­ç­‰é•¿åº¦æ–‡æœ¬è¿›è¡Œå¤šè½®æµ‹è¯•
    stress_texts = generate_texts_by_tokens(tokenizer, 256, 100)
    stress_results = []

    print("\n  è¿è¡Œ 10 è½®å‹åŠ›æµ‹è¯•...")
    all_batch_latencies = []
    for round_idx in range(10):
        result = benchmark_with_breakdown(
            tokenizer, model, stress_texts, device, dtype,
            512, 32, f"å‹åŠ›æµ‹è¯•-R{round_idx+1}"
        )
        stress_results.append(result)
        all_batch_latencies.extend(result.latencies)
        print(f"    è½®æ¬¡ {round_idx+1}: åå {result.throughput:.2f}/s")

    # æ±‡æ€»ç»Ÿè®¡
    combined_result = BenchmarkResult(
        name="å‹åŠ›æµ‹è¯•-æ±‡æ€»",
        text_count=sum(r.text_count for r in stress_results),
        batch_size=32,
        max_length=512,
        total_time_ms=sum(r.total_time_ms for r in stress_results),
        tokenize_time_ms=sum(r.tokenize_time_ms for r in stress_results),
        encode_time_ms=sum(r.encode_time_ms for r in stress_results),
        throughput=sum(r.text_count for r in stress_results) / (sum(r.total_time_ms for r in stress_results) / 1000),
        avg_latency_ms=sum(r.total_time_ms for r in stress_results) / sum(r.text_count for r in stress_results),
        latencies=all_batch_latencies,
        memory_mb=stress_results[-1].memory_mb,
        peak_memory_mb=max(r.peak_memory_mb for r in stress_results)
    )

    print_percentile_stats([combined_result])

    # ==================== æ±‡æ€»æŠ¥å‘Š ====================
    print("\n" + "="*100)
    print("  ğŸ“Š æ±‡æ€»æŠ¥å‘Š")
    print("="*100)

    print_result_table(all_results, "å…¨éƒ¨æµ‹è¯•ç»“æœ")
    print_latency_breakdown(all_results)
    print_percentile_stats(all_results)

    # æ¨èé…ç½®
    print("\n" + "="*100)
    print("  ğŸ’¡ ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®")
    print("="*100)

    if cuda_available:
        print(f"""
  åŸºäºæµ‹è¯•ç»“æœï¼Œæ¨èä»¥ä¸‹é…ç½®ï¼š

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ åœºæ™¯             â”‚ MaxLength â”‚ BatchSize â”‚ é¢„æœŸåå    â”‚ æ˜¾å­˜   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ çŸ­æ–‡æœ¬é«˜åå     â”‚ 512       â”‚ 64        â”‚ ~{length_results[0].throughput:.0f}/s      â”‚ <4GB   â”‚
  â”‚ é€šç”¨åœºæ™¯         â”‚ 1024      â”‚ 32        â”‚ ~{length_results[1].throughput:.0f}/s      â”‚ <8GB   â”‚
  â”‚ é•¿æ–‡æœ¬å¤„ç†       â”‚ 4096      â”‚ 8         â”‚ ~{length_results[2].throughput:.0f}/s       â”‚ <16GB  â”‚
  â”‚ æ··åˆçœŸå®åœºæ™¯     â”‚ 1536      â”‚ 32        â”‚ ~{mixed_results[1].throughput:.0f}/s      â”‚ <10GB  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  âš ï¸  æ³¨æ„äº‹é¡¹ï¼š
  1. ç”Ÿäº§ç¯å¢ƒå»ºè®® batch_size è®¾ä¸ºæµ‹è¯•å®‰å…¨å€¼çš„ 80%
  2. æ··åˆé•¿åº¦åœºæ™¯ä½¿ç”¨ padding="longest" å¯é¿å…ä¸å¿…è¦çš„è®¡ç®—
  3. P99 å»¶è¿Ÿæ˜¯ SLA ä¿éšœçš„å…³é”®æŒ‡æ ‡
  4. ç›‘æ§æ˜¾å­˜ä½¿ç”¨ï¼Œé¢„ç•™ 20% ç¼“å†²åº”å¯¹å³°å€¼
""")
    else:
        print("\n  âš ï¸  å½“å‰ä¸º CPU æ¨¡å¼ï¼Œå»ºè®®å¯ç”¨ GPU ä»¥è·å¾—æ›´å¥½æ€§èƒ½")

    # æ¸…ç†
    del model, tokenizer
    if cuda_available:
        torch.cuda.empty_cache()
    gc.collect()

    print("\n" + "="*100)
    print("  âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*100)


if __name__ == "__main__":
    main()
