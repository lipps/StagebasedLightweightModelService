#!/usr/bin/env python3
"""
GPU åŠŸèƒ½éªŒè¯è„šæœ¬
éªŒè¯ PyTorch 2.9.1 + RTX 5090 æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys

print("="*60)
print("  PyTorch GPU éªŒè¯")
print("="*60)

try:
    import torch
    print(f"\nâœ… PyTorch å¯¼å…¥æˆåŠŸ")
    print(f"   ç‰ˆæœ¬: {torch.__version__}")
except ImportError as e:
    print(f"\nâŒ PyTorch å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æ£€æŸ¥ CUDA å¯ç”¨æ€§
print(f"\n{'='*60}")
print("  CUDA çŠ¶æ€æ£€æŸ¥")
print("="*60)

cuda_available = torch.cuda.is_available()
print(f"\nCUDA å¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")

if not cuda_available:
    print("\nâš ï¸  CUDA ä¸å¯ç”¨ï¼Œå¯èƒ½çš„åŸå› ï¼š")
    print("  1. PyTorch ç¼–è¯‘æ—¶æœªåŒ…å« CUDA æ”¯æŒ")
    print("  2. NVIDIA é©±åŠ¨é—®é¢˜")
    print("  3. ç¯å¢ƒå˜é‡é…ç½®é—®é¢˜")
    sys.exit(1)

# CUDA è¯¦ç»†ä¿¡æ¯
print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
print(f"cuDNN å¯ç”¨: {'âœ… æ˜¯' if torch.backends.cudnn.enabled else 'âŒ å¦'}")

# GPU ä¿¡æ¯
print(f"\n{'='*60}")
print("  GPU è®¾å¤‡ä¿¡æ¯")
print("="*60)

gpu_count = torch.cuda.device_count()
print(f"\nGPU æ•°é‡: {gpu_count}")

for i in range(gpu_count):
    props = torch.cuda.get_device_properties(i)
    print(f"\nGPU {i}:")
    print(f"  åç§°: {torch.cuda.get_device_name(i)}")
    print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    print(f"  æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
    print(f"  å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")

# å†…å­˜çŠ¶æ€
print(f"\n{'='*60}")
print("  æ˜¾å­˜ä½¿ç”¨æƒ…å†µ (GPU 0)")
print("="*60)

print(f"\nå·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
print(f"å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB")
print(f"æœ€å¤§å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.2f} MB")

# ç®€å•è®¡ç®—æµ‹è¯•
print(f"\n{'='*60}")
print("  GPU è®¡ç®—æµ‹è¯•")
print("="*60)

try:
    print("\næµ‹è¯• 1: å¼ é‡åˆ›å»ºå’Œè½¬ç§»")
    x = torch.randn(1000, 1000).cuda()
    print("  âœ… å¼ é‡åˆ›å»ºæˆåŠŸ")

    print("\næµ‹è¯• 2: çŸ©é˜µä¹˜æ³• (FP32)")
    y = torch.randn(1000, 1000).cuda()
    z = torch.matmul(x, y)
    torch.cuda.synchronize()
    print(f"  âœ… çŸ©é˜µä¹˜æ³•æˆåŠŸï¼Œç»“æœå½¢çŠ¶: {z.shape}")

    print("\næµ‹è¯• 3: FP16 ç²¾åº¦")
    x_fp16 = x.half()
    y_fp16 = y.half()
    z_fp16 = torch.matmul(x_fp16, y_fp16)
    torch.cuda.synchronize()
    print(f"  âœ… FP16 è®¡ç®—æˆåŠŸï¼Œç»“æœå½¢çŠ¶: {z_fp16.shape}")

    print("\næµ‹è¯• 4: é‡Šæ”¾æ˜¾å­˜")
    del x, y, z, x_fp16, y_fp16, z_fp16
    torch.cuda.empty_cache()
    print("  âœ… æ˜¾å­˜é‡Šæ”¾æˆåŠŸ")

    print(f"\nå½“å‰æ˜¾å­˜: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")

except Exception as e:
    print(f"\nâŒ GPU è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
    sys.exit(1)

# RTX 5090 ç‰¹å®šæ£€æŸ¥
print(f"\n{'='*60}")
print("  RTX 5090 å…¼å®¹æ€§æ£€æŸ¥")
print("="*60)

props = torch.cuda.get_device_properties(0)
compute_cap = f"{props.major}.{props.minor}"
compute_cap_value = props.major * 10 + props.minor

print(f"\nè®¡ç®—èƒ½åŠ›: {compute_cap} (sm_{props.major}{props.minor})")

if compute_cap_value >= 120:  # sm_120 (RTX 5090)
    print("  âœ… RTX 5090 (sm_120) å®Œå…¨æ”¯æŒï¼")
    print("  âœ… å¯ä»¥ä½¿ç”¨æ‰€æœ‰ç°ä»£ CUDA ç‰¹æ€§")
elif compute_cap_value >= 90:
    print("  âœ… ç°ä»£ GPUï¼Œæ”¯æŒå¤§éƒ¨åˆ†ç‰¹æ€§")
elif compute_cap_value >= 80:
    print("  âš ï¸  è¾ƒæ—§çš„ GPU æ¶æ„")
else:
    print("  âš ï¸  éå¸¸æ—§çš„ GPU æ¶æ„ï¼Œå¯èƒ½æ€§èƒ½å—é™")

# æœ€ç»ˆæ€»ç»“
print(f"\n{'='*60}")
print("  éªŒè¯æ€»ç»“")
print("="*60)

print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
print("\nPyTorch 2.9.1 å·²æˆåŠŸé…ç½®ï¼Œæ”¯æŒï¼š")
print(f"  â€¢ {gpu_count} Ã— {torch.cuda.get_device_name(0)}")
print(f"  â€¢ è®¡ç®—èƒ½åŠ›: {compute_cap}")
print(f"  â€¢ CUDA {torch.version.cuda}")
print(f"  â€¢ æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB Ã— {gpu_count}")
print("\nğŸš€ ç°åœ¨å¯ä»¥å¯ç”¨ GPU æ¨¡å¼è·å¾—æœ€ä½³æ€§èƒ½ï¼")

print("\n" + "="*60)
