#!/usr/bin/env python3
"""
BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯• - å…¨é¢ç‰ˆ (10+ æµ‹è¯•ç”¨ä¾‹)

æµ‹è¯•çŸ©é˜µ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ç”¨ä¾‹ç»„           â”‚ å­ç”¨ä¾‹                                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 1. é•¿åº¦æ¢¯åº¦      â”‚ 128/256/512/1024/2048/4096 tokens (6ç»„)        â”‚
  â”‚ 2. Batch Size    â”‚ 8/16/32/64/128 (5ç»„)                            â”‚
  â”‚ 3. å®‰å…¨è¾¹ç•Œæ¢æµ‹  â”‚ max_length=512/1024/2048/4096 (4ç»„)            â”‚
  â”‚ 4. æ··åˆé•¿åº¦      â”‚ çŸ­+ä¸­/ä¸­+é•¿/çŸ­+ä¸­+é•¿ åˆ†å¸ƒ (3ç»„)                 â”‚
  â”‚ 5. ç²¾åº¦å¯¹æ¯”      â”‚ FP16 vs FP32 (2ç»„)                              â”‚
  â”‚ 6. å†·çƒ­å¯åŠ¨      â”‚ å†·å¯åŠ¨ vs çƒ­å¯åŠ¨å»¶è¿Ÿ (2ç»„)                      â”‚
  â”‚ 7. å¹¶å‘å‹åŠ›      â”‚ æŒç»­è´Ÿè½½ä¸‹çš„ P50/P90/P95/P99 (1ç»„)              â”‚
  â”‚ 8. çœŸå®æµæ¨¡æ‹Ÿ    â”‚ å˜é•¿è¯·æ±‚æµ tail latency (1ç»„)                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  æ€»è®¡: 24+ æµ‹è¯•ç”¨ä¾‹
"""

import gc
import os
import sys
import time
import random
import statistics
import json
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, field, asdict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import torch
from transformers import AutoTokenizer, AutoModel

# ==================== é…ç½® ====================

MODEL_ID = "/opt/bge-m3/models/bge-m3"
REPORT_FILE = "/opt/bge-m3/benchmark_report.json"

# ==================== æ•°æ®ç»“æ„ ====================

@dataclass
class BenchmarkResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    name: str
    category: str  # æµ‹è¯•ç±»åˆ«
    text_count: int
    batch_size: int
    max_length: int
    avg_tokens: float  # å¹³å‡ token æ•°
    total_time_ms: float
    tokenize_time_ms: float
    encode_time_ms: float
    throughput: float  # texts/sec
    tokens_per_sec: float  # tokens/sec
    avg_latency_ms: float
    latencies: List[float] = field(default_factory=list)
    memory_mb: float = 0.0
    peak_memory_mb: float = 0.0
    dtype: str = "fp16"


@dataclass
class LatencyStats:
    """å»¶è¿Ÿç»Ÿè®¡"""
    min_ms: float
    max_ms: float
    avg_ms: float
    p50_ms: float
    p90_ms: float
    p95_ms: float
    p99_ms: float
    std_ms: float


# ==================== å·¥å…·å‡½æ•° ====================

def generate_texts_by_tokens(tokenizer, target_tokens: int, count: int, variance: float = 0.1) -> Tuple[List[str], List[int]]:
    """ç”ŸæˆæŒ‡å®š token é•¿åº¦çš„æ–‡æœ¬

    Args:
        tokenizer: åˆ†è¯å™¨
        target_tokens: ç›®æ ‡ token æ•°
        count: æ–‡æœ¬æ•°é‡
        variance: é•¿åº¦å˜å¼‚ç³»æ•° (0.1 = Â±10%)

    Returns:
        (texts, actual_token_counts)
    """
    base_text = ("è¿™æ˜¯ä¸€æ®µç”¨äºæ€§èƒ½æµ‹è¯•çš„æ–‡æœ¬å†…å®¹ï¼ŒåŒ…å«äº†å¤šç§ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ã€‚"
                 "äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚"
                 "å¤§å‹è¯­è¨€æ¨¡å‹é€šè¿‡åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚")

    # ä¼°ç®—å­—ç¬¦/token æ¯”ç‡
    sample_tokens = len(tokenizer.encode(base_text, add_special_tokens=False))
    chars_per_token = len(base_text) / sample_tokens

    texts = []
    actual_lengths = []

    for i in range(count):
        # æ·»åŠ éšæœºå˜å¼‚
        var_factor = 1.0 + random.uniform(-variance, variance)
        adjusted_tokens = int(target_tokens * var_factor)
        target_chars = int(adjusted_tokens * chars_per_token * 1.1)

        # ç”Ÿæˆæ–‡æœ¬
        repeated = (base_text + f"[æ ·æœ¬{i}]") * (target_chars // len(base_text) + 1)
        text = repeated[:target_chars]
        texts.append(text)

        # è®°å½•å®é™…é•¿åº¦
        actual_len = len(tokenizer.encode(text, add_special_tokens=True))
        actual_lengths.append(actual_len)

    return texts, actual_lengths


def generate_mixed_distribution(tokenizer, count: int, distribution: Dict[int, float]) -> Tuple[List[str], List[int]]:
    """ç”Ÿæˆæ··åˆé•¿åº¦åˆ†å¸ƒçš„æ–‡æœ¬

    Args:
        distribution: {target_tokens: percentage}, e.g., {64: 0.4, 256: 0.4, 1024: 0.2}
    """
    texts = []
    lengths = []

    for target_tokens, pct in distribution.items():
        n = int(count * pct)
        batch_texts, batch_lengths = generate_texts_by_tokens(tokenizer, target_tokens, n)
        texts.extend(batch_texts)
        lengths.extend(batch_lengths)

    # è¡¥é½ä½™æ•°
    remaining = count - len(texts)
    if remaining > 0:
        first_target = list(distribution.keys())[0]
        extra_texts, extra_lengths = generate_texts_by_tokens(tokenizer, first_target, remaining)
        texts.extend(extra_texts)
        lengths.extend(extra_lengths)

    # æ‰“ä¹±é¡ºåº
    combined = list(zip(texts, lengths))
    random.shuffle(combined)
    texts, lengths = zip(*combined) if combined else ([], [])

    return list(texts), list(lengths)


def mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Mean Pooling"""
    mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)
    summed = (last_hidden_state * mask).sum(dim=1)
    counts = mask.sum(dim=1).clamp(min=1e-9)
    return summed / counts


def compute_latency_stats(latencies: List[float]) -> LatencyStats:
    """è®¡ç®—å»¶è¿Ÿç»Ÿè®¡"""
    if not latencies:
        return LatencyStats(0, 0, 0, 0, 0, 0, 0, 0)

    sorted_lat = sorted(latencies)
    n = len(sorted_lat)

    def pct(p: float) -> float:
        idx = min(int(n * p / 100), n - 1)
        return sorted_lat[idx]

    return LatencyStats(
        min_ms=min(latencies),
        max_ms=max(latencies),
        avg_ms=statistics.mean(latencies),
        p50_ms=pct(50),
        p90_ms=pct(90),
        p95_ms=pct(95),
        p99_ms=pct(99),
        std_ms=statistics.stdev(latencies) if len(latencies) > 1 else 0
    )


def get_gpu_memory_info() -> Dict[str, float]:
    """è·å– GPU æ˜¾å­˜ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return {}
    return {
        "allocated_mb": torch.cuda.memory_allocated() / 1024**2,
        "reserved_mb": torch.cuda.memory_reserved() / 1024**2,
        "peak_mb": torch.cuda.max_memory_allocated() / 1024**2,
        "total_mb": torch.cuda.get_device_properties(0).total_memory / 1024**2
    }


# ==================== æ ¸å¿ƒæµ‹è¯•å‡½æ•° ====================

@torch.inference_mode()
def benchmark_encode(
    tokenizer,
    model,
    texts: List[str],
    device: str,
    dtype: torch.dtype,
    max_length: int,
    batch_size: int,
    name: str = "test",
    category: str = "general"
) -> BenchmarkResult:
    """å¸¦å®Œæ•´æ—¶é—´æ‹†è§£çš„ç¼–ç åŸºå‡†æµ‹è¯•"""

    all_vecs = []
    batch_latencies = []
    total_tokenize_time = 0.0
    total_encode_time = 0.0
    total_tokens = 0

    # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()

    start_total = time.perf_counter()

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_start = time.perf_counter()

        # Tokenize
        tok_start = time.perf_counter()
        inputs = tokenizer(
            batch,
            padding="longest",
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        if device == "cuda":
            torch.cuda.synchronize()
        tok_end = time.perf_counter()
        total_tokenize_time += (tok_end - tok_start) * 1000

        # ç»Ÿè®¡ token æ•°
        total_tokens += inputs["attention_mask"].sum().item()

        # Encode
        enc_start = time.perf_counter()
        out = model(**inputs, return_dict=True)
        vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
        vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
        if device == "cuda":
            torch.cuda.synchronize()
        enc_end = time.perf_counter()
        total_encode_time += (enc_end - enc_start) * 1000

        all_vecs.extend(vecs.detach().float().cpu().tolist())

        batch_end = time.perf_counter()
        batch_latencies.append((batch_end - batch_start) * 1000)

    if device == "cuda":
        torch.cuda.synchronize()

    end_total = time.perf_counter()
    total_time_ms = (end_total - start_total) * 1000

    # æ˜¾å­˜ç»Ÿè®¡
    memory_mb = 0.0
    peak_memory_mb = 0.0
    if device == "cuda":
        memory_mb = torch.cuda.memory_allocated() / 1024**2
        peak_memory_mb = torch.cuda.max_memory_allocated() / 1024**2

    count = len(all_vecs)
    throughput = count / (total_time_ms / 1000) if total_time_ms > 0 else 0
    tokens_per_sec = total_tokens / (total_time_ms / 1000) if total_time_ms > 0 else 0
    avg_tokens = total_tokens / count if count > 0 else 0

    return BenchmarkResult(
        name=name,
        category=category,
        text_count=count,
        batch_size=batch_size,
        max_length=max_length,
        avg_tokens=avg_tokens,
        total_time_ms=total_time_ms,
        tokenize_time_ms=total_tokenize_time,
        encode_time_ms=total_encode_time,
        throughput=throughput,
        tokens_per_sec=tokens_per_sec,
        avg_latency_ms=total_time_ms / count if count > 0 else 0,
        latencies=batch_latencies,
        memory_mb=memory_mb,
        peak_memory_mb=peak_memory_mb,
        dtype="fp16" if dtype == torch.float16 else "fp32"
    )


def find_max_safe_batch(
    tokenizer,
    model,
    device: str,
    dtype: torch.dtype,
    max_length: int,
    safety_margin: float = 0.9
) -> Tuple[int, float, int]:
    """äºŒåˆ†æŸ¥æ‰¾æœ€å¤§å®‰å…¨ batch size

    Returns: (safe_batch, peak_memory_mb, absolute_max_batch)
    """
    if device != "cuda":
        return 256, 0.0, 256

    test_texts, _ = generate_texts_by_tokens(tokenizer, max_length, 512)

    safe_batch = 1
    safe_memory = 0.0
    low, high = 1, 512

    while low <= high:
        mid = (low + high) // 2

        try:
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

            batch = test_texts[:mid]
            inputs = tokenizer(batch, padding="longest", truncation=True,
                             max_length=max_length, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}

            with torch.inference_mode():
                out = model(**inputs, return_dict=True)
                vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
                vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
                _ = vecs.cpu()

            torch.cuda.synchronize()
            peak_mem = torch.cuda.max_memory_allocated() / 1024**2

            safe_batch = mid
            safe_memory = peak_mem
            low = mid + 1

        except torch.cuda.OutOfMemoryError:
            high = mid - 1
            torch.cuda.empty_cache()
        except Exception:
            high = mid - 1

    absolute_max = safe_batch
    safe_batch = int(safe_batch * safety_margin)

    return safe_batch, safe_memory, absolute_max


# ==================== æµ‹è¯•ç”¨ä¾‹ç»„ ====================

def test_length_gradient(tokenizer, model, device, dtype) -> List[BenchmarkResult]:
    """æµ‹è¯•ç»„1: é•¿åº¦æ¢¯åº¦æµ‹è¯• (6ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 1: é•¿åº¦æ¢¯åº¦æµ‹è¯• (128/256/512/1024/2048/4096 tokens)")
    print("="*100)

    configs = [
        {"tokens": 128,  "count": 500, "batch": 64, "max_len": 256},
        {"tokens": 256,  "count": 400, "batch": 64, "max_len": 512},
        {"tokens": 512,  "count": 300, "batch": 32, "max_len": 768},
        {"tokens": 1024, "count": 200, "batch": 32, "max_len": 1536},
        {"tokens": 2048, "count": 100, "batch": 16, "max_len": 2560},
        {"tokens": 4096, "count": 50,  "batch": 8,  "max_len": 4608},
    ]

    results = []
    for cfg in configs:
        print(f"\n  ğŸ“Š æµ‹è¯• ~{cfg['tokens']} tokens...")
        texts, lengths = generate_texts_by_tokens(tokenizer, cfg["tokens"], cfg["count"])
        avg_len = sum(lengths) / len(lengths)
        print(f"     ç”Ÿæˆ {len(texts)} æ¡æ–‡æœ¬ï¼Œå¹³å‡ {avg_len:.0f} tokens")

        result = benchmark_encode(
            tokenizer, model, texts, device, dtype,
            cfg["max_len"], cfg["batch"],
            f"~{cfg['tokens']}tok", "length_gradient"
        )
        results.append(result)
        print(f"     âœ… åå: {result.throughput:.1f}/s | "
              f"Tokenåå: {result.tokens_per_sec:.0f} tok/s | "
              f"å»¶è¿Ÿ: {result.avg_latency_ms:.2f}ms")

    return results


def test_batch_size_scaling(tokenizer, model, device, dtype) -> List[BenchmarkResult]:
    """æµ‹è¯•ç»„2: Batch Size æ‰©å±•æ€§ (5ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 2: Batch Size æ‰©å±•æ€§æµ‹è¯• (8/16/32/64/128)")
    print("="*100)

    # ä½¿ç”¨å›ºå®šé•¿åº¦æ–‡æœ¬
    texts, _ = generate_texts_by_tokens(tokenizer, 256, 500)

    results = []
    for batch_size in [8, 16, 32, 64, 128]:
        print(f"\n  ğŸ“Š æµ‹è¯• batch_size={batch_size}...")

        result = benchmark_encode(
            tokenizer, model, texts, device, dtype,
            512, batch_size,
            f"B{batch_size}", "batch_scaling"
        )
        results.append(result)
        print(f"     âœ… åå: {result.throughput:.1f}/s | å³°å€¼æ˜¾å­˜: {result.peak_memory_mb:.0f}MB")

    return results


def test_safe_batch_boundary(tokenizer, model, device, dtype) -> List[Dict]:
    """æµ‹è¯•ç»„3: å®‰å…¨ Batch è¾¹ç•Œæ¢æµ‹ (4ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 3: FP16 å®‰å…¨ Batch Size è¾¹ç•Œæ¢æµ‹")
    print("="*100)

    results = []
    for max_len in [512, 1024, 2048, 4096]:
        print(f"\n  ğŸ” æ¢æµ‹ max_length={max_len}...")
        safe_batch, peak_mem, abs_max = find_max_safe_batch(
            tokenizer, model, device, dtype, max_len
        )
        results.append({
            "max_length": max_len,
            "safe_batch": safe_batch,
            "absolute_max": abs_max,
            "peak_memory_mb": peak_mem
        })
        print(f"     âœ… å®‰å…¨: {safe_batch} | æé™: {abs_max} | å³°å€¼: {peak_mem:.0f}MB")

    return results


def test_mixed_distribution(tokenizer, model, device, dtype) -> List[BenchmarkResult]:
    """æµ‹è¯•ç»„4: æ··åˆé•¿åº¦åˆ†å¸ƒ (3ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 4: æ··åˆé•¿åº¦åˆ†å¸ƒæµ‹è¯•")
    print("="*100)

    distributions = [
        {"name": "çŸ­+ä¸­ (50/50)", "dist": {64: 0.5, 256: 0.5}, "max_len": 512},
        {"name": "ä¸­+é•¿ (60/40)", "dist": {256: 0.6, 1024: 0.4}, "max_len": 1536},
        {"name": "çŸ­+ä¸­+é•¿ (40/40/20)", "dist": {64: 0.4, 256: 0.4, 1024: 0.2}, "max_len": 1536},
    ]

    results = []
    for cfg in distributions:
        print(f"\n  ğŸ“Š æµ‹è¯• {cfg['name']}...")
        texts, lengths = generate_mixed_distribution(tokenizer, 300, cfg["dist"])

        result = benchmark_encode(
            tokenizer, model, texts, device, dtype,
            cfg["max_len"], 32,
            cfg["name"], "mixed_distribution"
        )
        results.append(result)

        # è®¡ç®— tail latency
        stats = compute_latency_stats(result.latencies)
        print(f"     âœ… åå: {result.throughput:.1f}/s | P95: {stats.p95_ms:.1f}ms | P99: {stats.p99_ms:.1f}ms")

    return results


def test_precision_comparison(tokenizer, model_fp16, model_fp32, device) -> List[BenchmarkResult]:
    """æµ‹è¯•ç»„5: FP16 vs FP32 ç²¾åº¦å¯¹æ¯” (2ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 5: FP16 vs FP32 ç²¾åº¦å¯¹æ¯”")
    print("="*100)

    texts, _ = generate_texts_by_tokens(tokenizer, 256, 200)
    results = []

    # FP16
    print("\n  ğŸ“Š æµ‹è¯• FP16...")
    result_fp16 = benchmark_encode(
        tokenizer, model_fp16, texts, device, torch.float16,
        512, 32, "FP16", "precision"
    )
    results.append(result_fp16)
    print(f"     âœ… åå: {result_fp16.throughput:.1f}/s | æ˜¾å­˜: {result_fp16.peak_memory_mb:.0f}MB")

    # FP32
    print("\n  ğŸ“Š æµ‹è¯• FP32...")
    result_fp32 = benchmark_encode(
        tokenizer, model_fp32, texts, device, torch.float32,
        512, 32, "FP32", "precision"
    )
    results.append(result_fp32)
    print(f"     âœ… åå: {result_fp32.throughput:.1f}/s | æ˜¾å­˜: {result_fp32.peak_memory_mb:.0f}MB")

    speedup = result_fp16.throughput / result_fp32.throughput if result_fp32.throughput > 0 else 0
    mem_ratio = result_fp16.peak_memory_mb / result_fp32.peak_memory_mb if result_fp32.peak_memory_mb > 0 else 0
    print(f"\n  ğŸ“ˆ FP16 åŠ é€Ÿæ¯”: {speedup:.2f}x | æ˜¾å­˜èŠ‚çœ: {(1-mem_ratio)*100:.1f}%")

    return results


def test_cold_vs_warm(tokenizer, model, device, dtype) -> List[BenchmarkResult]:
    """æµ‹è¯•ç»„6: å†·å¯åŠ¨ vs çƒ­å¯åŠ¨ (2ç»„)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 6: å†·å¯åŠ¨ vs çƒ­å¯åŠ¨å»¶è¿Ÿå¯¹æ¯”")
    print("="*100)

    texts, _ = generate_texts_by_tokens(tokenizer, 256, 100)
    results = []

    # å†·å¯åŠ¨ (æ¸…ç©ºç¼“å­˜å)
    print("\n  ğŸ“Š æµ‹è¯•å†·å¯åŠ¨...")
    if device == "cuda":
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

    result_cold = benchmark_encode(
        tokenizer, model, texts[:10], device, dtype,
        512, 10, "å†·å¯åŠ¨", "startup"
    )
    results.append(result_cold)
    print(f"     âœ… é¦–æ‰¹å»¶è¿Ÿ: {result_cold.total_time_ms:.2f}ms")

    # çƒ­å¯åŠ¨ (æ¨¡å‹å·²é¢„çƒ­)
    print("\n  ğŸ“Š æµ‹è¯•çƒ­å¯åŠ¨...")
    # å…ˆé¢„çƒ­
    _ = benchmark_encode(tokenizer, model, texts[:50], device, dtype, 512, 32, "warmup", "")

    result_warm = benchmark_encode(
        tokenizer, model, texts[:10], device, dtype,
        512, 10, "çƒ­å¯åŠ¨", "startup"
    )
    results.append(result_warm)
    print(f"     âœ… é¦–æ‰¹å»¶è¿Ÿ: {result_warm.total_time_ms:.2f}ms")

    speedup = result_cold.total_time_ms / result_warm.total_time_ms if result_warm.total_time_ms > 0 else 0
    print(f"\n  ğŸ“ˆ çƒ­å¯åŠ¨åŠ é€Ÿ: {speedup:.2f}x")

    return results


def test_sustained_pressure(tokenizer, model, device, dtype) -> Tuple[BenchmarkResult, LatencyStats]:
    """æµ‹è¯•ç»„7: æŒç»­å‹åŠ›æµ‹è¯• (æ”¶é›† P50/P90/P95/P99)"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 7: æŒç»­å‹åŠ›æµ‹è¯• (20è½®è¿­ä»£)")
    print("="*100)

    texts, _ = generate_texts_by_tokens(tokenizer, 256, 100)
    all_latencies = []
    throughputs = []

    print("\n  è¿è¡Œ 20 è½®å‹åŠ›æµ‹è¯•...")
    for i in range(20):
        result = benchmark_encode(
            tokenizer, model, texts, device, dtype,
            512, 32, f"Round{i+1}", "pressure"
        )
        all_latencies.extend(result.latencies)
        throughputs.append(result.throughput)

        if (i + 1) % 5 == 0:
            print(f"     è½®æ¬¡ {i+1}/20: åå {result.throughput:.1f}/s")

    # æ±‡æ€»ç»Ÿè®¡
    stats = compute_latency_stats(all_latencies)
    avg_throughput = statistics.mean(throughputs)
    throughput_std = statistics.stdev(throughputs) if len(throughputs) > 1 else 0

    combined = BenchmarkResult(
        name="å‹åŠ›æµ‹è¯•æ±‡æ€»",
        category="pressure",
        text_count=len(texts) * 20,
        batch_size=32,
        max_length=512,
        avg_tokens=256,
        total_time_ms=sum(r.latencies[0] for r in [result] * 20 if r.latencies),
        tokenize_time_ms=0,
        encode_time_ms=0,
        throughput=avg_throughput,
        tokens_per_sec=avg_throughput * 256,
        avg_latency_ms=stats.avg_ms,
        latencies=all_latencies
    )

    print(f"\n  ğŸ“Š å‹åŠ›æµ‹è¯•æ±‡æ€»:")
    print(f"     å¹³å‡åå: {avg_throughput:.1f} Â± {throughput_std:.1f} /s")
    print(f"     P50: {stats.p50_ms:.2f}ms | P90: {stats.p90_ms:.2f}ms | "
          f"P95: {stats.p95_ms:.2f}ms | P99: {stats.p99_ms:.2f}ms")

    return combined, stats


def test_realworld_stream(tokenizer, model, device, dtype) -> Tuple[BenchmarkResult, LatencyStats]:
    """æµ‹è¯•ç»„8: çœŸå®è¯·æ±‚æµæ¨¡æ‹Ÿ"""
    print("\n" + "="*100)
    print("  æµ‹è¯•ç»„ 8: çœŸå®è¯·æ±‚æµæ¨¡æ‹Ÿ (å˜é•¿è¾“å…¥)")
    print("="*100)

    # æ¨¡æ‹ŸçœŸå®åˆ†å¸ƒ: æŒ‡æ•°åˆ†å¸ƒçš„è¯·æ±‚é•¿åº¦
    lengths = []
    for _ in range(500):
        # æŒ‡æ•°åˆ†å¸ƒï¼Œå¤§éƒ¨åˆ†çŸ­ï¼Œå°‘é‡é•¿
        base = random.expovariate(1/200)  # å¹³å‡200 tokens
        length = max(32, min(int(base), 2048))
        lengths.append(length)

    # ç”Ÿæˆå¯¹åº”é•¿åº¦çš„æ–‡æœ¬
    texts = []
    actual_lengths = []
    for target_len in lengths:
        t, l = generate_texts_by_tokens(tokenizer, target_len, 1)
        texts.extend(t)
        actual_lengths.extend(l)

    print(f"\n  ç”Ÿæˆ {len(texts)} æ¡å˜é•¿æ–‡æœ¬:")
    print(f"     æœ€çŸ­: {min(actual_lengths)} tokens")
    print(f"     æœ€é•¿: {max(actual_lengths)} tokens")
    print(f"     å¹³å‡: {sum(actual_lengths)/len(actual_lengths):.0f} tokens")

    # æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šå° batch é€ä¸ªå¤„ç†
    all_latencies = []
    start = time.perf_counter()

    for i in range(0, len(texts), 16):  # batch=16 æ¨¡æ‹Ÿåœ¨çº¿è¯·æ±‚
        batch = texts[i:i+16]
        batch_start = time.perf_counter()

        inputs = tokenizer(batch, padding="longest", truncation=True,
                          max_length=2560, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.inference_mode():
            out = model(**inputs, return_dict=True)
            vecs = mean_pooling(out.last_hidden_state, inputs["attention_mask"])
            vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)
            _ = vecs.cpu()

        if device == "cuda":
            torch.cuda.synchronize()

        batch_end = time.perf_counter()
        all_latencies.append((batch_end - batch_start) * 1000)

    total_time = (time.perf_counter() - start) * 1000
    stats = compute_latency_stats(all_latencies)

    result = BenchmarkResult(
        name="çœŸå®è¯·æ±‚æµ",
        category="realworld",
        text_count=len(texts),
        batch_size=16,
        max_length=2560,
        avg_tokens=sum(actual_lengths)/len(actual_lengths),
        total_time_ms=total_time,
        tokenize_time_ms=0,
        encode_time_ms=0,
        throughput=len(texts) / (total_time / 1000),
        tokens_per_sec=sum(actual_lengths) / (total_time / 1000),
        avg_latency_ms=total_time / len(texts),
        latencies=all_latencies
    )

    print(f"\n  ğŸ“Š çœŸå®æµæµ‹è¯•ç»“æœ:")
    print(f"     æ€»åå: {result.throughput:.1f} texts/s")
    print(f"     Tokenåå: {result.tokens_per_sec:.0f} tokens/s")
    print(f"     Tail Latency - P95: {stats.p95_ms:.2f}ms | P99: {stats.p99_ms:.2f}ms | Max: {stats.max_ms:.2f}ms")

    return result, stats


# ==================== æŠ¥å‘Šç”Ÿæˆ ====================

def print_summary_table(results: List[BenchmarkResult], title: str):
    """æ‰“å°æ±‡æ€»è¡¨æ ¼"""
    print(f"\n{'='*120}")
    print(f"  {title}")
    print('='*120)

    print(f"\n{'ç”¨ä¾‹':<20} {'ç±»åˆ«':<15} {'æ•°é‡':>6} {'Batch':>6} {'MaxLen':>7} "
          f"{'AvgTok':>7} {'åå(/s)':>10} {'Tok/s':>10} {'å»¶è¿Ÿ(ms)':>10} {'æ˜¾å­˜(MB)':>10}")
    print("-" * 120)

    for r in results:
        print(f"{r.name:<20} {r.category:<15} {r.text_count:>6} {r.batch_size:>6} {r.max_length:>7} "
              f"{r.avg_tokens:>7.0f} {r.throughput:>10.1f} {r.tokens_per_sec:>10.0f} "
              f"{r.avg_latency_ms:>10.2f} {r.peak_memory_mb:>10.1f}")


def print_tokenize_breakdown(results: List[BenchmarkResult]):
    """æ‰“å° Tokenize æ—¶é—´æ‹†è§£"""
    print(f"\n{'='*120}")
    print("  Tokenize vs Encode æ—¶é—´æ‹†è§£")
    print('='*120)

    print(f"\n{'ç”¨ä¾‹':<25} {'æ€»æ—¶é—´(ms)':>12} {'Tokenize(ms)':>14} {'Encode(ms)':>12} "
          f"{'Tokå æ¯”':>10} {'Encå æ¯”':>10}")
    print("-" * 120)

    for r in results:
        if r.total_time_ms > 0:
            tok_pct = r.tokenize_time_ms / r.total_time_ms * 100
            enc_pct = r.encode_time_ms / r.total_time_ms * 100
            print(f"{r.name:<25} {r.total_time_ms:>12.2f} {r.tokenize_time_ms:>14.2f} "
                  f"{r.encode_time_ms:>12.2f} {tok_pct:>9.1f}% {enc_pct:>9.1f}%")


def print_percentile_table(results: List[BenchmarkResult]):
    """æ‰“å°ç™¾åˆ†ä½å»¶è¿Ÿè¡¨"""
    print(f"\n{'='*120}")
    print("  æ‰¹æ¬¡å»¶è¿Ÿåˆ†ä½æ•° (P50/P90/P95/P99)")
    print('='*120)

    print(f"\n{'ç”¨ä¾‹':<25} {'Min(ms)':>10} {'P50(ms)':>10} {'P90(ms)':>10} "
          f"{'P95(ms)':>10} {'P99(ms)':>10} {'Max(ms)':>10} {'StdDev':>10}")
    print("-" * 120)

    for r in results:
        if r.latencies:
            stats = compute_latency_stats(r.latencies)
            print(f"{r.name:<25} {stats.min_ms:>10.2f} {stats.p50_ms:>10.2f} "
                  f"{stats.p90_ms:>10.2f} {stats.p95_ms:>10.2f} {stats.p99_ms:>10.2f} "
                  f"{stats.max_ms:>10.2f} {stats.std_ms:>10.2f}")


def print_safe_batch_table(safe_batch_results: List[Dict]):
    """æ‰“å°å®‰å…¨ Batch è¡¨"""
    print(f"\n{'='*80}")
    print("  FP16 å®‰å…¨ Batch Size æ¨è (å« 10% å®‰å…¨è¾¹ç•Œ)")
    print('='*80)

    print(f"\n  {'MaxLength':>12} â”‚ {'SafeBatch':>12} â”‚ {'AbsoluteMax':>12} â”‚ {'PeakMem(MB)':>14}")
    print(f"  {'-'*12}â”€â”¼â”€{'-'*12}â”€â”¼â”€{'-'*12}â”€â”¼â”€{'-'*14}")

    for r in safe_batch_results:
        print(f"  {r['max_length']:>12} â”‚ {r['safe_batch']:>12} â”‚ {r['absolute_max']:>12} â”‚ {r['peak_memory_mb']:>14.1f}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    print("="*120)
    print("  BGE-M3 GPU æ€§èƒ½åŸºå‡†æµ‹è¯• - å…¨é¢ç‰ˆ (24+ æµ‹è¯•ç”¨ä¾‹)")
    print("="*120)
    print(f"  æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # æ£€æŸ¥ç¯å¢ƒ
    cuda_available = torch.cuda.is_available()
    print(f"\n  CUDA å¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")

    if cuda_available:
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  æ˜¾å­˜æ€»é‡: {gpu_mem:.1f} GB")

    device = "cuda" if cuda_available else "cpu"

    # åŠ è½½æ¨¡å‹
    print(f"\n  åŠ è½½æ¨¡å‹...")
    start = time.time()
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, local_files_only=True)

    # FP16 æ¨¡å‹
    model_fp16 = AutoModel.from_pretrained(MODEL_ID, torch_dtype=torch.float16, local_files_only=True)
    model_fp16.to(device).eval()

    # FP32 æ¨¡å‹ (ç”¨äºå¯¹æ¯”)
    model_fp32 = AutoModel.from_pretrained(MODEL_ID, torch_dtype=torch.float32, local_files_only=True)
    model_fp32.to(device).eval()

    print(f"  âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({time.time() - start:.2f}s)")

    # é¢„çƒ­
    print("\n  æ¨¡å‹é¢„çƒ­...")
    _ = benchmark_encode(tokenizer, model_fp16, ["é¢„çƒ­"] * 16, device, torch.float16, 128, 16, "warmup", "")
    print("  âœ… é¢„çƒ­å®Œæˆ")

    # ==================== è¿è¡Œæ‰€æœ‰æµ‹è¯• ====================
    all_results = []

    # æµ‹è¯•ç»„ 1: é•¿åº¦æ¢¯åº¦ (6ç»„)
    results_length = test_length_gradient(tokenizer, model_fp16, device, torch.float16)
    all_results.extend(results_length)

    # æµ‹è¯•ç»„ 2: Batch Size æ‰©å±• (5ç»„)
    results_batch = test_batch_size_scaling(tokenizer, model_fp16, device, torch.float16)
    all_results.extend(results_batch)

    # æµ‹è¯•ç»„ 3: å®‰å…¨è¾¹ç•Œæ¢æµ‹ (4ç»„)
    safe_batch_results = test_safe_batch_boundary(tokenizer, model_fp16, device, torch.float16)

    # æµ‹è¯•ç»„ 4: æ··åˆåˆ†å¸ƒ (3ç»„)
    results_mixed = test_mixed_distribution(tokenizer, model_fp16, device, torch.float16)
    all_results.extend(results_mixed)

    # æµ‹è¯•ç»„ 5: ç²¾åº¦å¯¹æ¯” (2ç»„)
    results_precision = test_precision_comparison(tokenizer, model_fp16, model_fp32, device)
    all_results.extend(results_precision)

    # æµ‹è¯•ç»„ 6: å†·çƒ­å¯åŠ¨ (2ç»„)
    results_startup = test_cold_vs_warm(tokenizer, model_fp16, device, torch.float16)
    all_results.extend(results_startup)

    # æµ‹è¯•ç»„ 7: æŒç»­å‹åŠ› (1ç»„)
    result_pressure, pressure_stats = test_sustained_pressure(tokenizer, model_fp16, device, torch.float16)
    all_results.append(result_pressure)

    # æµ‹è¯•ç»„ 8: çœŸå®æµ (1ç»„)
    result_realworld, realworld_stats = test_realworld_stream(tokenizer, model_fp16, device, torch.float16)
    all_results.append(result_realworld)

    # ==================== æ±‡æ€»æŠ¥å‘Š ====================
    print("\n" + "="*120)
    print("  ğŸ“Š å®Œæ•´æµ‹è¯•æŠ¥å‘Š")
    print("="*120)

    print_summary_table(all_results, "å…¨éƒ¨æµ‹è¯•ç»“æœæ±‡æ€»")
    print_tokenize_breakdown([r for r in all_results if r.tokenize_time_ms > 0])
    print_percentile_table([r for r in all_results if r.latencies])
    print_safe_batch_table(safe_batch_results)

    # ==================== ç”Ÿäº§å»ºè®® ====================
    print("\n" + "="*120)
    print("  ğŸ’¡ ç”Ÿäº§ç¯å¢ƒé…ç½®å»ºè®®")
    print("="*120)

    if cuda_available:
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_throughput = max(results_length, key=lambda x: x.throughput)
        best_token_throughput = max(results_length, key=lambda x: x.tokens_per_sec)

        print(f"""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                          æ¨èé…ç½®çŸ©é˜µ                                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ åœºæ™¯               â”‚ MaxLength â”‚ BatchSize â”‚ é¢„æœŸåå      â”‚ P99å»¶è¿Ÿ   â”‚ æ˜¾å­˜  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ é«˜ååçŸ­æ–‡æœ¬       â”‚ 256       â”‚ {safe_batch_results[0]['safe_batch']:<10}â”‚ ~{results_length[0].throughput:.0f}/s       â”‚ <50ms     â”‚ <2GB  â”‚
  â”‚ é€šç”¨å¹³è¡¡           â”‚ 512       â”‚ {safe_batch_results[0]['safe_batch']:<10}â”‚ ~{results_length[1].throughput:.0f}/s       â”‚ <80ms     â”‚ <4GB  â”‚
  â”‚ é•¿æ–‡æœ¬å¤„ç†         â”‚ 2048      â”‚ {safe_batch_results[2]['safe_batch']:<10}â”‚ ~{results_length[4].throughput:.0f}/s       â”‚ <200ms    â”‚ <14GB â”‚
  â”‚ è¶…é•¿æ–‡æœ¬           â”‚ 4096      â”‚ {safe_batch_results[3]['safe_batch']:<10}â”‚ ~{results_length[5].throughput:.0f}/s        â”‚ <400ms    â”‚ <20GB â”‚
  â”‚ æ··åˆçœŸå®åœºæ™¯       â”‚ 1536      â”‚ 32        â”‚ ~{results_mixed[2].throughput:.0f}/s       â”‚ <100ms    â”‚ <3GB  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  âš ï¸  å…³é”®æ³¨æ„äº‹é¡¹:

  1. OOM é˜²æŠ¤:
     - ç”Ÿäº§ç¯å¢ƒ batch_size è®¾ä¸ºä¸Šè¡¨ "SafeBatch" çš„ 80%
     - å®ç°è¯·æ±‚é˜Ÿåˆ—é™æµï¼Œé˜²æ­¢çªå‘æµé‡

  2. Tokenize ä¼˜åŒ–:
     - çŸ­æ–‡æœ¬åœºæ™¯ Tokenize å æ¯”é«˜è¾¾ {results_length[0].tokenize_time_ms/results_length[0].total_time_ms*100:.1f}%
     - è€ƒè™‘é¢„ç¼–è¯‘é«˜é¢‘æŸ¥è¯¢æˆ–ä½¿ç”¨ tokenizer ç¼“å­˜

  3. Tail Latency æ§åˆ¶:
     - å‹åŠ›æµ‹è¯• P99: {pressure_stats.p99_ms:.2f}ms
     - çœŸå®æµ P99: {realworld_stats.p99_ms:.2f}ms
     - æ··åˆé•¿åº¦åœºæ™¯ P99 æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®æŒ‰é•¿åº¦åˆ†æ¡¶å¤„ç†

  4. ç²¾åº¦é€‰æ‹©:
     - FP16 ç›¸æ¯” FP32 åŠ é€Ÿ {results_precision[0].throughput/results_precision[1].throughput:.2f}x
     - æ˜¾å­˜èŠ‚çœ {(1-results_precision[0].peak_memory_mb/results_precision[1].peak_memory_mb)*100:.1f}%
     - ç²¾åº¦æŸå¤±å¯å¿½ç•¥ï¼Œæ¨èä½¿ç”¨ FP16

  5. é¢„çƒ­ç­–ç•¥:
     - å†·å¯åŠ¨å»¶è¿Ÿ: {results_startup[0].total_time_ms:.2f}ms
     - çƒ­å¯åŠ¨å»¶è¿Ÿ: {results_startup[1].total_time_ms:.2f}ms
     - æœåŠ¡å¯åŠ¨æ—¶åŠ¡å¿…æ‰§è¡Œé¢„çƒ­
""")

    # ä¿å­˜æŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "device": device,
        "gpu_name": torch.cuda.get_device_name(0) if cuda_available else "N/A",
        "results": [asdict(r) for r in all_results],
        "safe_batch": safe_batch_results,
        "pressure_stats": asdict(pressure_stats) if hasattr(pressure_stats, '__dict__') else {},
        "realworld_stats": asdict(realworld_stats) if hasattr(realworld_stats, '__dict__') else {}
    }

    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False, default=str)
    print(f"\n  ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {REPORT_FILE}")

    # æ¸…ç†
    del model_fp16, model_fp32, tokenizer
    if cuda_available:
        torch.cuda.empty_cache()
    gc.collect()

    print("\n" + "="*120)
    print(f"  âœ… å…¨éƒ¨ {len(all_results)} ç»„æµ‹è¯•å®Œæˆï¼")
    print("="*120)


if __name__ == "__main__":
    main()
